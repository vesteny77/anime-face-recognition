{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9690ed47-bfac-4f2f-ac26-ddf3b3f02ccf",
   "metadata": {},
   "source": [
    "# Visualize filters and feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9f857-398b-489e-ba26-52b3a9e8f285",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7c858-2471-4ec4-bbf4-ee5844fbeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os, os.path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from pathlib import Path\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# to display filter and feater map images \n",
    "import matplotlib.pyplot as plt\n",
    "# import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34584c39-d378-46c7-b158-a4277daf6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780d0cb-464c-459a-95ab-1b4983fbbf8e",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38ff8a-4431-44d8-987f-0cf74d40bf71",
   "metadata": {},
   "source": [
    "### Define Attribute Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004ba9d-ec75-4826-87f3-2116bad3b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "def print_opts(opts):\n",
    "    \"\"\"\n",
    "    Print all the parameters in opts before training starts.\n",
    "    \"\"\"\n",
    "    print('=' * 79)\n",
    "    print('Opts'.center(79))\n",
    "    print('-' * 79)\n",
    "    for key in opts.__dict__:\n",
    "        if opts.__dict__[key]:\n",
    "            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(79))\n",
    "    print('=' * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008ba96-3f8e-4525-bd34-498fbb217672",
   "metadata": {},
   "source": [
    "### Define Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b65d29-41bd-4696-a621-aece16268df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_loader(opts):\n",
    "    \"\"\"\n",
    "    Retunrs a dictionary containing two key-value pairs:\n",
    "        'train': training dataloader\n",
    "        'test': validation dataloader\n",
    "    \"\"\"\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(opts.image_size),\n",
    "            transforms.CenterCrop(opts.crop_size),\n",
    "            transforms.RandomHorizontalFlip(p=opts.flip_prob),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(opts.image_size),\n",
    "            transforms.CenterCrop(opts.crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    dataset_path = \"dataset/\"\n",
    "    image_datasets = {category: datasets.ImageFolder(os.path.join(dataset_path, category),\n",
    "                                                     data_transforms[category])\n",
    "                      for category in ['train', 'test']}\n",
    "    \n",
    "    dataloader_dict = {category: torch.utils.data.DataLoader(image_datasets[category],\n",
    "                                                         batch_size=opts.batch_size,\n",
    "                                                         shuffle=True,\n",
    "                                                         num_workers=opts.num_workers)\n",
    "                   for category in ['train', 'test']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "    print(f\"dataset_sizes = {dataset_sizes}\")\n",
    "    print(f\"Total number of images = {dataset_sizes['train'] + dataset_sizes['test']}\")\n",
    "    class_names = image_datasets['train'].classes\n",
    "    # print(f\"class_name 0 = {class_names[0]}\")\n",
    "    \n",
    "    return dataloader_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81652635-770c-478c-bb5f-ccd2ecc7d97f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f0269-532c-4396-8283-2d7f69902cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3aeb65-4681-4942-9b80-bd74067835eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def googlenet_train_loop(model, criterion, optimizer, scheduler, epochs):\n",
    "    \n",
    "    model_weights = [[] for _ in range(epochs)] \n",
    "    conv_layers = [[] for _ in range(epochs)] \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "\n",
    "        # Training the model\n",
    "        model.train()\n",
    "        counter = 0\n",
    "        for inputs, labels in tqdm(dataloader_dict['train']):\n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Clear optimizers\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output, aux_outputs = model(inputs)\n",
    "            # Loss\n",
    "            loss = criterion(output, labels)\n",
    "            # Calculate gradients (backpropogation)\n",
    "            loss.backward()\n",
    "            # Adjust parameters based on gradients\n",
    "            optimizer.step()\n",
    "            # Add the loss to the training set's rnning loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        # update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Get the average loss for the entire epoch\n",
    "        train_loss = train_loss/len(dataloader_dict['train'].dataset)\n",
    "\n",
    "        # Print training loss\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "        #################################################################################\n",
    "        # copied from https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/#########\n",
    "        # Refer to the [6] in reference section \n",
    "        model_children = list(model.children())\n",
    "        for i in range(len(model_children)):\n",
    "            if type(model_children[i]) == torchvision.models.inception.BasicConv2d:\n",
    "                model_weights[epoch].append(model_children[i].conv.weight)\n",
    "                # conv_layers[epoch].append(model_children[i].conv)\n",
    "            # elif type(model_children[i]) == nn.Sequential:\n",
    "            #     for j in range(len(model_children[i])):\n",
    "            #         for child in model_children[i][j].children():\n",
    "            #             if type(child) == nn.Conv2d:\n",
    "            #                 model_weights[epoch].append(child.weight)\n",
    "            #                 conv_layers[epoch].append(child)\n",
    "        ##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ded8e4-eea7-4e61-8aa5-5b79b6875440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_train_loop(model, criterion, optimizer, scheduler, epochs):\n",
    "    \n",
    "    model_weights = [[] for _ in range(epochs)] \n",
    "    conv_layers = [[] for _ in range(epochs)] \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "\n",
    "        # Training the model\n",
    "        model.train()\n",
    "        counter = 0\n",
    "        for inputs, labels in tqdm(dataloader_dict['train']):\n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Clear optimizers\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            # output, aux_outputs = model(inputs)\n",
    "            output = model(inputs)\n",
    "            # Loss\n",
    "            loss = criterion(output, labels)\n",
    "            # Calculate gradients (backpropogation)\n",
    "            loss.backward()\n",
    "            # Adjust parameters based on gradients\n",
    "            optimizer.step()\n",
    "            # Add the loss to the training set's rnning loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        # Get the average loss for the entire epoch\n",
    "        train_loss = train_loss/len(dataloader_dict['train'].dataset)\n",
    "\n",
    "        # Print training loss\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "        #################################################################################\n",
    "        # copied from https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/#########\n",
    "        # Refer to the [6] in reference section\n",
    "        # for each epoch, remember all weights\n",
    "        \n",
    "        model_children = list(model.children())\n",
    "        for i in range(len(model_children)):\n",
    "            if type(model_children[i]) == nn.Conv2d:\n",
    "                model_weights[epoch].append(model_children[i].weight)\n",
    "                conv_layers[epoch].append(model_children[i])\n",
    "            elif type(model_children[i]) == nn.Sequential:\n",
    "                for j in range(len(model_children[i])):\n",
    "                    for child in model_children[i][j].children():\n",
    "                        if type(child) == nn.Conv2d:\n",
    "                            model_weights[epoch].append(child.weight)\n",
    "                            conv_layers[epoch].append(child)\n",
    "    return model_weights, conv_layers\n",
    "        ##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0ee5e-e3ce-4dd2-9e72-a620ab1a8493",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8dfc5-6702-4919-9944-791caa6b3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AngularPenaltySMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, loss_type='arcface', eps=1e-7, s=None, m=None):\n",
    "        '''\n",
    "        Angular Penalty Softmax Loss\n",
    "        Three 'loss_types' available: ['arcface', 'sphereface', 'cosface']\n",
    "        These losses are described in the following papers: \n",
    "        \n",
    "        ArcFace: https://arxiv.org/abs/1801.07698\n",
    "        SphereFace: https://arxiv.org/abs/1704.08063\n",
    "        CosFace/Ad Margin: https://arxiv.org/abs/1801.05599\n",
    "        '''\n",
    "        super(AngularPenaltySMLoss, self).__init__()\n",
    "        loss_type = loss_type.lower()\n",
    "        assert loss_type in  ['arcface', 'sphereface', 'cosface']\n",
    "        if loss_type == 'arcface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 0.5 if not m else m\n",
    "        if loss_type == 'sphereface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 1.35 if not m else m\n",
    "        if loss_type == 'cosface':\n",
    "            self.s = 30.0 if not s else s\n",
    "            self.m = 0.4 if not m else m\n",
    "        self.loss_type = loss_type\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "        assert len(x) == len(labels)\n",
    "        assert torch.min(labels) >= 0\n",
    "        assert torch.max(labels) < self.out_features\n",
    "        \n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, p=2, dim=1)\n",
    "\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "\n",
    "        wf = self.fc(x)\n",
    "        if self.loss_type == 'cosface':\n",
    "            numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n",
    "        if self.loss_type == 'arcface':\n",
    "            numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)) + self.m)\n",
    "        if self.loss_type == 'sphereface':\n",
    "            numerator = self.s * torch.cos(self.m * torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)))\n",
    "\n",
    "        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y+1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d3a13-a827-4db4-af56-a973a32591b6",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656aead2-ec6f-43eb-87cc-25d095228bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opts):\n",
    "    train_model, criterion, optimizer, scheduler = None, None, None, None\n",
    "    \n",
    "    if opts.model == 'GoogLeNet':\n",
    "        train_model = models.inception_v3(pretrained=True)\n",
    "    else:\n",
    "        train_model = models.resnet50(pretrained=True)\n",
    "        \n",
    "        \n",
    "    for param in train_model.parameters():\n",
    "        param.requires_grad = opts.requires_grad\n",
    "        \n",
    "    num_in_features = train_model.fc.in_features\n",
    "    train_model.fc = nn.Linear(num_in_features, num_classes)\n",
    "    \n",
    "    # Handle the auxilary net\n",
    "    # aux_num_ftrs = train_model.AuxLogits.fc.in_features\n",
    "    # train_model.AuxLogits.fc = nn.Linear(aux_num_ftrs, num_classes)\n",
    "    \n",
    "    # Handle the primary net\n",
    "    # primary_num_ftrs = train_model.fc.in_features\n",
    "    # train_model.fc = nn.Linear(primary_num_ftrs, num_classes)\n",
    "    \n",
    "    if opts.loss_function == 'CrossEntropyLoss':\n",
    "        train_model.fc = nn.Linear(num_in_features, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = AngularPenaltySMLoss(num_in_features, num_classes, loss_type=opts.loss_function)\n",
    "        \n",
    "    if opts.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(train_model.parameters(), lr=opts.lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.Adam(train_model.parameters(), lr=opts.lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opts.step_size, gamma=opts.gamma)\n",
    "    \n",
    "    # move model to GPU\n",
    "    train_model = train_model.to(device)\n",
    "    print(f\"Model moved to GPU: {device}\")\n",
    "    \n",
    "    # model_weights conv_layers = googlenet_train_loop(train_model, criterion, optimizer, scheduler, opts.epochs)\n",
    "    model_weights, conv_layers = resnet_train_loop(train_model, criterion, optimizer, scheduler, opts.epochs)\n",
    "    return model_weights, conv_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02652e4f-e6f3-4ed0-903b-08c64d988007",
   "metadata": {},
   "source": [
    "### Define filter and feature map displaying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014dfbf-0e3a-4890-8a56-f671a5eb2d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################copied from https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/#########################\n",
    "# Refer to the [6] in reference section\n",
    "def displayFeatureMap(img_name, img, epoch): # which image? which epoch?\n",
    "    results = [conv_layers[epoch][0](img)]\n",
    "    for i in range(1, len(conv_layers[epoch])):\n",
    "        # pass the result from the last layer to the next layer\n",
    "        results.append(conv_layers[epoch][i](results[-1]))\n",
    "    \n",
    "    for num_layer in range(len(results)):\n",
    "        plt.figure(figsize=(30, 30))\n",
    "        layer_viz = results[num_layer][ :, :, :]\n",
    "        layer_viz = layer_viz.data\n",
    "        # print(layer_viz.size())\n",
    "        for i, filter in enumerate(layer_viz):\n",
    "            if i == 64: # we will visualize only 8x8 blocks from each layer\n",
    "                break\n",
    "            plt.subplot(8, 8, i + 1)\n",
    "            plt.imshow(filter.cpu().clone().numpy(), cmap='gray')\n",
    "            plt.axis(\"off\")\n",
    "        # print(f\"Saving layer {num_layer} feature maps...\")\n",
    "        plt.savefig(f\"outputs/img{img_name}-epoch{epoch}-layer{num_layer}.png\")\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a512a7-171d-40cf-a300-a8746d995dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################copied from https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/#########################\n",
    "# Refer to the [6] in reference section\n",
    "def displayFilter(epoch, layer):\n",
    "    plt.figure(figsize=(20, 17))\n",
    "    for i, filter in enumerate(model_weights[epoch][layer]):\n",
    "        plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "        plt.imshow(filter[0, :, :].detach().cpu().clone().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'outputs/filter-epoch{epoch}-layer{layer}.png')\n",
    "    plt.show()\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0be253-e108-4a79-9922-e2db681abffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img(img_class, img_name, opts):\n",
    "    \"\"\"\n",
    "    Given image class and image name, return a tuple (\"img_class_img_name\", tensor of the image)\n",
    "    \"\"\"\n",
    "    transform_func = transforms.Compose([\n",
    "        transforms.Resize(opts.image_size),\n",
    "        transforms.CenterCrop(opts.crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    root_path = 'dataset/train'\n",
    "    img_pil = Image.open(os.path.join(root_path, str(img_class), str(img_name) + \".jpg\"))\n",
    "    return \"_\".join([str(img_class), str(img_name)]), transform_func(img_pil).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1ceb9-6ef0-43d7-b3ed-f0a70fb91b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('50_6',\n",
       " tensor([[[ 2.0777,  2.0777,  2.0777,  ...,  0.3309,  0.3309,  0.3652],\n",
       "          [ 2.1119,  2.1119,  2.1119,  ...,  0.3309,  0.3309,  0.3309],\n",
       "          [ 2.0948,  2.0948,  2.0948,  ...,  0.3309,  0.3309,  0.3309],\n",
       "          ...,\n",
       "          [-0.4397, -0.4397,  1.2728,  ...,  0.5878,  0.5878,  0.5878],\n",
       "          [-0.4397, -0.4397,  1.3927,  ...,  0.5878,  0.5878,  0.5878],\n",
       "          [-0.4397, -0.5424,  1.1700,  ...,  0.5878,  0.5878,  0.5878]],\n",
       " \n",
       "         [[ 2.2710,  2.2710,  2.2710,  ...,  0.2052,  0.2052,  0.2402],\n",
       "          [ 2.2360,  2.2360,  2.2360,  ...,  0.2052,  0.2052,  0.2052],\n",
       "          [ 2.2185,  2.2185,  2.2185,  ...,  0.2052,  0.2052,  0.2052],\n",
       "          ...,\n",
       "          [-0.3200, -0.3200,  1.4307,  ...,  0.7304,  0.7304,  0.7304],\n",
       "          [-0.3375, -0.3200,  1.5532,  ...,  0.7304,  0.7304,  0.7304],\n",
       "          [-0.3375, -0.4426,  1.3081,  ...,  0.7304,  0.7304,  0.7304]],\n",
       " \n",
       "         [[ 1.7163,  1.7163,  1.7163,  ..., -0.1138, -0.1138, -0.0790],\n",
       "          [ 1.6988,  1.6988,  1.6988,  ..., -0.1138, -0.1138, -0.1138],\n",
       "          [ 1.6814,  1.6814,  1.6640,  ..., -0.1138, -0.1138, -0.1138],\n",
       "          ...,\n",
       "          [-0.0964, -0.0964,  1.6640,  ...,  0.9494,  0.9494,  0.9494],\n",
       "          [-0.0964, -0.0964,  1.7860,  ...,  0.9494,  0.9494,  0.9494],\n",
       "          [-0.0441, -0.1487,  1.5768,  ...,  0.9494,  0.9494,  0.9494]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_args = AttrDict()\n",
    "vis_args_dict = {\n",
    "    'image_size': 256,  # 299\n",
    "    'crop_size': 224,  # 299\n",
    "}\n",
    "\n",
    "vis_args.update(vis_args_dict)\n",
    "transform_img(50, 6, vis_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e838ef5-2ac1-4bf5-baec-1a1903108ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "dataset_sizes = {'train': 69849, 'test': 17435}\n",
      "Total number of images = 87284\n",
      "===============================================================================\n",
      "                                      Opts                                     \n",
      "-------------------------------------------------------------------------------\n",
      "                             image_size: 256                                   \n",
      "                              crop_size: 224                                   \n",
      "                              flip_prob: 0.3                                   \n",
      "                             batch_size: 32                                    \n",
      "                            num_workers: 2                                     \n",
      "                                 epochs: 10                                    \n",
      "                                     lr: 0.001                                 \n",
      "                                  model: resnet                                \n",
      "                          loss_function: CrossEntropyLoss                      \n",
      "                              optimizer: Adam                                  \n",
      "                              step_size: 7                                     \n",
      "                                  gamma: 0.1                                   \n",
      "===============================================================================\n",
      "Model moved to GPU: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2183/2183 [08:00<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 3.102345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2183/2183 [08:04<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.355407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2183/2183 [08:00<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 1.043403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 927/2183 [03:25<04:31,  4.63it/s]"
     ]
    }
   ],
   "source": [
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    'image_size': 256,  # 299\n",
    "    'crop_size': 224,  # 299\n",
    "    'flip_prob': 0.3,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 2,\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-3,\n",
    "    'model': 'resnet',\n",
    "    'loss_function': 'CrossEntropyLoss', # 'CrossEntropyLoss', 'arcface', 'sphereface', 'cosface'\n",
    "    'optimizer': 'Adam',  # 'Adam', 'SGD'\n",
    "    'step_size': 7,  # for learning rate scheduler\n",
    "    'gamma': 0.1,  # for learning rate scheduler\n",
    "    'requires_grad': False\n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "print(\"Loading dataset...\")\n",
    "dataloader_dict = get_anime_loader(args)\n",
    "print_opts(args)\n",
    "model_weights, conv_layers = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d3e6a-6725-44a0-aa6f-683fd9a0659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_args = AttrDict()\n",
    "vis_args_dict = {\n",
    "    'image_size': 256,  # 299\n",
    "    'crop_size': 224,  # 299\n",
    "}\n",
    "\n",
    "vis_args.update(vis_args_dict)\n",
    "img_class_img_name, tensor_of_the_image = transform_img(50, 6, vis_args)\n",
    "displayFeatureMap(img_class_img_name, tensor_of_the_image, 0)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
